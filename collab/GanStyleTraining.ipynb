{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.multiprocessing import Process\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from random import Random\n",
    "\n",
    "WORLD_SIZE = 2\n",
    "NUM_EPOCHS = 50\n",
    "TRAINING_RECORD_INTERVAL = 25\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partion(object):\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(244),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "class DataPartitioner(object):\n",
    "\n",
    "    def __init__(self, data, seed = 8675309):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        \n",
    "        # todo generalize this\n",
    "        p1_indexes, p2_indexes = [], []\n",
    "        for idx in range(len(data)):\n",
    "            if data[idx][1] < 5:\n",
    "                p1_indexes.append(idx)\n",
    "            else:\n",
    "                p2_indexes.append(idx)\n",
    "\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        rng.shuffle(p1_indexes)\n",
    "        rng.shuffle(p2_indexes)\n",
    "\n",
    "        self.partitions.append(p1_indexes)\n",
    "        self.partitions.append(p2_indexes)\n",
    "\n",
    "    def use(self, partion):\n",
    "        return Partion(self.data, self.partitions[partion])\n",
    "\n",
    "def partition_dataset(world_size, train = True):\n",
    "\n",
    "    dataset = datasets.CIFAR10('./data', download = True, \n",
    "       train = train, \n",
    "       transform = (transform_train if train else transform_test))\n",
    "\n",
    "    partition = DataPartitioner(dataset)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(partition, \n",
    "                                        batch_size = 64, \n",
    "                                        shuffle = True)\n",
    "    return train_set, 64\n",
    "\n",
    "def average_gradients(model, rank):\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "\n",
    "        other_model_param = torch.zeros(1)\n",
    "        # pass to rank 1\n",
    "        if rank == 0:\n",
    "            dist.send(param.grad.data, dst = 1)\n",
    "        else:\n",
    "            dist.recv(tenosr = other_model_param, src = 0)\n",
    "\n",
    "        # pass to rank 0\n",
    "        if rank == 0:\n",
    "            dist.recv(tensor = other_model_param, src = 1)\n",
    "        else:\n",
    "            dist.send(param.grad.data, dst = 1)\n",
    "\n",
    "        param.grad.data = torch.log(param.grad.data) - torch.log(1 - param.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rank, size, model, criterion, optimizer):\n",
    "    \n",
    "    torch.manual_seed(8675309)\n",
    "    training_set, bsz = partition_dataset(WORLD_SIZE)\n",
    "    testing_set, bsz = partition_dataset(WORLD_SIZE, train = False)\n",
    "\n",
    "    model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    # Set up record holder and testing set for only the master node\n",
    "    if rank == 0:\n",
    "        training_accuracy = []\n",
    "        testing_accuracy = []\n",
    "        \n",
    "        imagenet_data_test = datasets.CIFAR10('./data', download = True, train = False)\n",
    "        testing_size = len(imagenet_data_test)\n",
    "        del imagenet_data_test\n",
    "        \n",
    "\n",
    "    for epoch_idx in range(NUM_EPOCHS):\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(training_set):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            if batch_idx % TRAINING_RECORD_INTERVAL == 0:\n",
    "              print('Rank %d\\tEpoch: %d\\tIterval: %d\\tAccuracy : %d%%' % (\n",
    "                  rank,\n",
    "                  epoch_idx,\n",
    "                  batch_idx,\n",
    "                  100 * correct / (BATCH_SIZE / WORLD_SIZE)))\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "            if rank == 0:\n",
    "              if batch_idx % TRAINING_RECORD_INTERVAL == 0:\n",
    "                training_accuracy.append(100 * correct / (BATCH_SIZE / WORLD_SIZE))\n",
    "\n",
    "            average_gradients(model, rank)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # After each epoch record testing results on master node\n",
    "        testing_correct = 0\n",
    "        for idx, (inputs, labels) in enumerate(testing_set):\n",
    "\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            testing_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        testing_correct = torch.tensor(testing_correct).cuda()\n",
    "        # wait till all processes have finished the epoch\n",
    "        dist.barrier()\n",
    "\n",
    "        recv = dist.all_reduce(testing_correct, op = dist.reduce_op.SUM)\n",
    "            \n",
    "        if rank == 0:\n",
    "            print('Epoch: %d\\tAccuracy: %d %%' % (epoch_idx, 100 * testing_correct.data.item() / testing_size))\n",
    "            testing_accuracy.append(100 * testing_correct / testing_size)\n",
    "        \n",
    "        dist.barrier()\n",
    "\n",
    "\n",
    "    if rank == 0:\n",
    "        np.save('/content/drive/My Drive/Colab Notebooks/Results/GAN Style Training/training_accuracy.npy', training_accuracy)\n",
    "        np.save('/content/drive/My Drive/Colab Notebooks/Results/GAN Style Training/testing_accuracy.npy', testing_accuracy)\n",
    "        #torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/Results/Parallel Control/model_control.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(rank, size, model, criterion, optimizer, fn, backend = \"nccl\"):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(backend, rank = rank, world_size = size)\n",
    "    fn(rank, size, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = []\n",
    "model = models.alexnet(num_classes = 10)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n",
    "\n",
    "for rank in range(WORLD_SIZE):\n",
    "    p = Process(target = init_process, args = (rank, WORLD_SIZE, model, criterion, optimizer, run))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(\"Execution Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiasedDataParallelNetwork",
   "language": "python",
   "name": "biaseddataparallelnetwork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-candidate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}