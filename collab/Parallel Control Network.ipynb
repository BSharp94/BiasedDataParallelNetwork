{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.optim as optim\nfrom torch.multiprocessing import Process\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom random import Random\n\nWORLD_SIZE = 2\nNUM_EPOCHS = 50\nTRAINING_RECORD_INTERVAL = 25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Partition(object):\n    \n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes, seed = 8675309):\n        self.data = data\n        self.partions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partion):\n        return Partition(self.data, self.partions[partion])\n\ndef partition_dataset(testing = False):\n    transform_train = transforms.Compose([\n            transforms.RandomResizedCrop(244),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n\n\n    transform_test = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    dataset = datasets.CIFAR10('./data', download = True, train = (not testing), \n        transform = (transform_train if not testing else transform_test))\n\n    size = dist.get_world_size()\n    bsz = 128 // float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=int(bsz),\n                                         shuffle=True)\n    return train_set, bsz\n\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n        param.grad.data /= size"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run(rank, size, model, criterion, optimizer):\n    \n    torch.manual_seed(8675309)\n    training_set, bsz = partition_dataset()\n    testing_set, bsz = partition_dataset(testing = True)\n\n    model.cuda()\n    criterion = criterion.cuda()\n\n    # Set up record holder and testing set for only the master node\n    if rank == 0:\n        training_accuracy = []\n        testing_accuracy = []\n        \n        imagenet_data_test = datasets.CIFAR10('./data', download = True, train = False)\n        testing_size = len(imagenet_data_test)\n        del imagenet_data_test\n        \n\n    for epoch_idx in range(NUM_EPOCHS):\n\n        for batch_idx, (data, target) in enumerate(training_set):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            outputs = model(data)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            correct = (predicted == target).sum().item()\n            loss = criterion(outputs, target)\n\n            if batch_idx % TRAINING_RECORD_INTERVAL == 0:\n              print('Rank %d\\tEpoch: %d\\tIterval: %d\\tAccuracy : %d%%' % (\n                  rank,\n                  epoch_idx,\n                  batch_idx,\n                  100 * correct / 64))\n\n            loss.backward()\n        \n            if rank == 0:\n              if batch_idx % TRAINING_RECORD_INTERVAL == 0:\n                training_accuracy.append(100 * correct / 64)\n\n            average_gradients(model)\n            optimizer.step()\n        \n        # After each epoch record testing results on master node\n        testing_correct = 0\n        for idx, (inputs, labels) in enumerate(testing_set):\n\n            inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n\n            _, predicted = torch.max(outputs.data, 1)\n            testing_correct += (predicted == labels).sum().item()\n    \n        testing_correct = torch.tensor(testing_correct)\n        # wait till all processes have finished the epoch\n        dist.barrier()\n\n        recv = dist.all_reduce(testing_correct, op = dist.reduce_op.SUM)\n            \n        if rank == 0:\n            print('Epoch: %d\\tAccuracy: %d %%' % (epoch_idx, 100 * testing_correct.data.item() / testing_size))\n            testing_accuracy.append(100 * testing_correct / testing_size)\n        \n        dist.barrier()\n\n\n    if rank == 0:\n        np.save('/content/drive/My Drive/Colab Notebooks/Results/Parallel Control/training_accuracy.npy', training_accuracy)\n        np.save('/content/drive/My Drive/Colab Notebooks/Results/Parallel Control/testing_accuracy.npy', testing_accuracy)\n        torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/Results/Parallel Control/model_control.pt')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def init_process(rank, size, model, criterion, optimizer, fn, backend = \"gloo\"):\n    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    dist.init_process_group(backend, rank = rank, world_size = size)\n    fn(rank, size, model, criterion, optimizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "processes = []\nmodel = models.alexnet(num_classes = 10)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n\nfor rank in range(WORLD_SIZE):\n    p = Process(target = init_process, args = (rank, WORLD_SIZE, model, criterion, optimizer, run))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()\n\nprint(\"Execution Finished\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiasedDataParallelNetwork",
   "language": "python",
   "name": "biaseddataparallelnetwork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
